{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachanabn20/CoLI-Dravidian_2025/blob/main/CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwPe_HJ-tVaP",
        "outputId": "b65799bd-0b0d-4fcd-b5b0-c7c5da4e522d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwLw9O0KtwzR",
        "outputId": "aa0b6750-92b7-4b1a-ab19-343ee798901f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.6.0)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn_crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import re\n",
        "import sklearn_crfsuite\n",
        "import string\n",
        "from sklearn_crfsuite import metrics"
      ],
      "metadata": {
        "id": "FPqQmLKagaI-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    word_counts = Counter()\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            word_counts[word] += 1\n",
        "    vocab = {word: idx+2 for idx, word in enumerate(word_counts)}\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "vp3LpBCRgaGP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentences_from_df(df, has_tags=True):\n",
        "    sentences, current = [], []\n",
        "    for index, row in df.iterrows():\n",
        "        word = str(row['Word'])\n",
        "        tag = str(row['Tag']) if has_tags else None\n",
        "        if 'ID' in df.columns and row['ID'] == 1.0 and index != 0:\n",
        "            if current: sentences.append(current)\n",
        "            current = []\n",
        "        current.append((word, tag))\n",
        "    if current:\n",
        "        sentences.append(current)\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "9ULmy_pzgZ_X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word': word,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[:3]': word[:3],\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'len(word)': len(word),\n",
        "        'ispunct': word in string.punctuation\n",
        "    }\n",
        "\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:word[:3]': word1[:3],\n",
        "            '-1:word[-3:]': word1[-3:],\n",
        "            '-1:word.isdigit()': word1.isdigit()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:word[:3]': word1[:3],\n",
        "            '+1:word[-3:]': word1[-3:],\n",
        "            '+1:word.isdigit()': word1.isdigit()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent): return [word2features(sent, i) for i in range(len(sent))]\n",
        "def sent2labels(sent): return [label for token, label in sent]\n"
      ],
      "metadata": {
        "id": "JbJUK0l9gZ8m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example file triplets (train, val, test)\n",
        "file_triples = [\n",
        "    ('kan_train.csv', 'kan_val.csv', 'kan_test.csv'),\n",
        "    ('mal_train(1).csv', 'mal_val(1).csv', 'mal_test.csv'),\n",
        "    ('tl_train.csv', 'tl_val.csv', 'tl_test.csv'),\n",
        "    ('tm_train.csv', 'tm_val.csv', 'tm_test.csv'),\n",
        "    ('tulu_train.csv', 'tulu_val.csv', 'tulu_test.csv')\n",
        "]\n",
        "\n",
        "dataframes = []\n",
        "for train_file, val_file, test_file in file_triples:\n",
        "    train_df = pd.read_csv(f'/content/drive/MyDrive/Train_and_development_data/Train and development data/{train_file}')\n",
        "    val_df = pd.read_csv(f'/content/drive/MyDrive/Train_and_development_data/Train and development data/{val_file}')\n",
        "    test_df = pd.read_csv(f'/content/drive/MyDrive/Test_Data_without_labels/Test Data without labels/{test_file}')\n",
        "    dataframes.append((train_df, val_df, test_df, train_file, val_file, test_file))\n"
      ],
      "metadata": {
        "id": "BFlREH-Bi81x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_df, val_df, test_df, train_file, val_file, test_file in dataframes:\n",
        "    print(f\"\\n Processing {train_file}, {val_file}, {test_file}\")\n",
        "\n",
        "    # Prepare structured sequences\n",
        "    train_sents = create_sentences_from_df(train_df, has_tags=True)\n",
        "    val_sents = create_sentences_from_df(val_df, has_tags=True)\n",
        "    test_sents = create_sentences_from_df(test_df, has_tags=False)\n",
        "\n",
        "    # Features & Labels\n",
        "    X_train = [sent2features(s) for s in train_sents]\n",
        "    y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "    X_val = [sent2features(s) for s in val_sents]\n",
        "    y_val = [sent2labels(s) for s in val_sents]\n",
        "\n",
        "    X_test = [sent2features(s) for s in test_sents]\n",
        "\n",
        "    # CRF Model\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm='lbfgs',\n",
        "        c1=0.1, c2=0.1,\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True\n",
        "    )\n",
        "\n",
        "    print(\"Training...\")\n",
        "    crf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation\n",
        "    val_pred = crf.predict(X_val)\n",
        "    print(f\"\\nValidation Report for {val_file}\")\n",
        "    print(metrics.flat_classification_report(y_val, val_pred, digits=3))\n",
        "\n",
        "    # Predict on test\n",
        "    test_preds = crf.predict(X_test)\n",
        "    test_labels_flat = [tag for seq in test_preds for tag in seq]\n",
        "\n",
        "    # Save predictions\n",
        "    if len(test_labels_flat) == len(test_df):\n",
        "        language = train_file.split('_')[0]\n",
        "        test_df['Tag'] = test_labels_flat\n",
        "        output_path = f'predictions_{language}.csv'\n",
        "        test_df[['Word', 'Tag']].to_csv(output_path, index=False)\n",
        "        print(f\"Test predictions saved to: {output_path}\")\n",
        "    else:\n",
        "        print(f\"Prediction mismatch in length for {test_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjdnLQzvi8z1",
        "outputId": "ba5a3144-86a5-4970-8a16-247fb4b16917"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Processing kan_train.csv, kan_val.csv, kan_test.csv\n",
            "Training...\n",
            "\n",
            "Validation Report for kan_val.csv\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          en      0.935     0.971     0.953       922\n",
            "          kn      0.889     0.925     0.907       546\n",
            "    location      1.000     1.000     1.000         2\n",
            "       mixed      0.976     0.705     0.818       176\n",
            "        name      0.923     0.480     0.632        50\n",
            "       other      0.645     0.851     0.734        47\n",
            "         sym      0.996     1.000     0.998       273\n",
            "\n",
            "    accuracy                          0.924      2016\n",
            "   macro avg      0.909     0.847     0.863      2016\n",
            "weighted avg      0.928     0.924     0.922      2016\n",
            "\n",
            "Test predictions saved to: predictions_kan_crf_val.csv\n",
            "\n",
            " Processing mal_train(1).csv, mal_val(1).csv, mal_test.csv\n",
            "Training...\n",
            "\n",
            "Validation Report for mal_val(1).csv\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     ENGLISH      0.908     0.941     0.924       407\n",
            "   MALAYALAM      0.907     0.983     0.943       860\n",
            "       MIXED      0.585     0.393     0.471        61\n",
            "        NAME      0.884     0.872     0.878       149\n",
            "      NUMBER      0.971     0.767     0.857        43\n",
            "       OTHER      0.932     0.734     0.821       263\n",
            "       PLACE      0.667     0.500     0.571         8\n",
            "         SYM      0.991     1.000     0.995       217\n",
            "\n",
            "    accuracy                          0.911      2008\n",
            "   macro avg      0.856     0.774     0.808      2008\n",
            "weighted avg      0.908     0.911     0.906      2008\n",
            "\n",
            "Test predictions saved to: predictions_mal_crf_val.csv\n",
            "\n",
            " Processing tl_train.csv, tl_val.csv, tl_test.csv\n",
            "Training...\n",
            "\n",
            "Validation Report for tl_val.csv\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     ENGLISH      0.743     0.950     0.834       301\n",
            "       MIXED      0.667     0.062     0.114        32\n",
            "        NAME      1.000     0.364     0.533        11\n",
            "      NUMBER      1.000     0.400     0.571         5\n",
            "       OTHER      0.391     0.173     0.240        52\n",
            "         SYM      1.000     1.000     1.000        57\n",
            "      TELUGU      0.854     0.614     0.714        57\n",
            "\n",
            "    accuracy                          0.767       515\n",
            "   macro avg      0.808     0.509     0.572       515\n",
            "weighted avg      0.751     0.767     0.725       515\n",
            "\n",
            "Test predictions saved to: predictions_tl_crf_val.csv\n",
            "\n",
            " Processing tm_train.csv, tm_val.csv, tm_test.csv\n",
            "Training...\n",
            "\n",
            "Validation Report for tm_val.csv\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Other      0.000     0.000     0.000         1\n",
            "          en      0.906     0.879     0.893       496\n",
            "        name      0.766     0.738     0.752       160\n",
            "         sym      1.000     1.000     1.000       183\n",
            "          tm      0.901     0.939     0.920      1000\n",
            "        tmen      0.821     0.701     0.757       144\n",
            "\n",
            "    accuracy                          0.896      1984\n",
            "   macro avg      0.732     0.709     0.720      1984\n",
            "weighted avg      0.894     0.896     0.894      1984\n",
            "\n",
            "Test predictions saved to: predictions_tm_crf_val.csv\n",
            "\n",
            " Processing tulu_train.csv, tulu_val.csv, tulu_test.csv\n",
            "Training...\n",
            "\n",
            "Validation Report for tulu_val.csv\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English      0.939     0.889     0.913       742\n",
            "     Kannada      0.755     0.700     0.726       273\n",
            "    Location      0.917     0.805     0.857        41\n",
            "       Mixed      0.897     0.456     0.605        57\n",
            "        Name      0.844     0.681     0.754       135\n",
            "       Other      0.587     0.518     0.550        85\n",
            "        Tulu      0.846     0.933     0.887      1251\n",
            "         sym      1.000     1.000     1.000       422\n",
            "\n",
            "    accuracy                          0.877      3006\n",
            "   macro avg      0.848     0.748     0.787      3006\n",
            "weighted avg      0.877     0.877     0.874      3006\n",
            "\n",
            "Test predictions saved to: predictions_tulu_crf_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66fae6a8",
        "outputId": "74c6c2ef-373f-415b-e4d2-5f1af0e3d7b4"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "labeled_test_files = {\n",
        "    'kan': \"/content/drive/MyDrive/CoLI-Dravidian/test_ref_knn.csv\",\n",
        "    'mal': \"/content/drive/MyDrive/CoLI-Dravidian/test_ref_mal.csv\",\n",
        "    'tl': \"/content/drive/MyDrive/CoLI-Dravidian/test_ref_tl.csv\",\n",
        "    'tm': \"/content/drive/MyDrive/CoLI-Dravidian/test_ref_tm.csv\",\n",
        "    'tulu': \"/content/drive/MyDrive/CoLI-Dravidian/test_ref_tulu.csv\"\n",
        "}\n",
        "\n",
        "for language, labeled_test_full_path in labeled_test_files.items():\n",
        "    prediction_file = f'predictions_{language}_crf_val.csv'\n",
        "\n",
        "    try:\n",
        "        # Load predictions\n",
        "        predictions_df = pd.read_csv(prediction_file)\n",
        "\n",
        "        # Load labeled test data\n",
        "        labeled_test_df = pd.read_csv(labeled_test_full_path)\n",
        "\n",
        "        # Ensure the lengths match before comparison\n",
        "        if len(predictions_df) == len(labeled_test_df):\n",
        "            print(f\"Classification Report for {language}:\")\n",
        "            print(classification_report(labeled_test_df['Tag'], predictions_df['Tag']))\n",
        "        else:\n",
        "            print(f\"Length mismatch for {language}: Predictions have {len(predictions_df)} rows, Labeled test has {len(labeled_test_df)} rows.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Could not find prediction file {prediction_file} or labeled test file for {language}. Please check file paths and names.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing files for {language}: {e}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for kan:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          en       0.97      0.98      0.97      1204\n",
            "          kn       0.88      0.94      0.91       289\n",
            "    location       0.80      0.80      0.80         5\n",
            "       mixed       0.97      0.77      0.86       141\n",
            "        name       0.91      0.76      0.83        55\n",
            "       other       0.85      0.85      0.85       117\n",
            "         sym       1.00      1.00      1.00       264\n",
            "\n",
            "    accuracy                           0.95      2075\n",
            "   macro avg       0.91      0.87      0.89      2075\n",
            "weighted avg       0.95      0.95      0.95      2075\n",
            "\n",
            "Classification Report for mal:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     ENGLISH       0.90      0.85      0.88       380\n",
            "   MALAYALAM       0.87      0.96      0.91       938\n",
            "       MIXED       0.69      0.28      0.40        88\n",
            "        NAME       0.74      0.70      0.72       158\n",
            "      NUMBER       0.98      0.86      0.91        50\n",
            "       OTHER       0.62      0.61      0.62       164\n",
            "       PLACE       1.00      0.62      0.77         8\n",
            "         SYM       1.00      1.00      1.00       211\n",
            "\n",
            "    accuracy                           0.86      1997\n",
            "   macro avg       0.85      0.74      0.78      1997\n",
            "weighted avg       0.85      0.86      0.85      1997\n",
            "\n",
            "Classification Report for tl:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     ENGLISH       0.92      0.97      0.94       238\n",
            "       MIXED       1.00      0.75      0.86         4\n",
            "        NAME       0.94      1.00      0.97        15\n",
            "      NUMBER       1.00      1.00      1.00         5\n",
            "       OTHER       1.00      0.16      0.27        19\n",
            "       PLACE       0.00      0.00      0.00         2\n",
            "         SYM       1.00      1.00      1.00        63\n",
            "      TELUGU       0.90      0.93      0.92       148\n",
            "\n",
            "    accuracy                           0.93       494\n",
            "   macro avg       0.84      0.73      0.74       494\n",
            "weighted avg       0.93      0.93      0.91       494\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for tm:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Location       0.00      0.00      0.00         9\n",
            "       Other       0.14      0.12      0.13        16\n",
            "          en       0.89      0.91      0.90       534\n",
            "        name       0.73      0.68      0.70       139\n",
            "         sym       1.00      0.91      0.95       230\n",
            "          tm       0.89      0.93      0.91       986\n",
            "        tmen       0.79      0.72      0.75       152\n",
            "\n",
            "    accuracy                           0.88      2066\n",
            "   macro avg       0.63      0.61      0.62      2066\n",
            "weighted avg       0.87      0.88      0.88      2066\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for tulu:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       0.94      0.90      0.92       813\n",
            "     Kannada       0.81      0.69      0.74       372\n",
            "    Location       0.97      0.62      0.76        56\n",
            "       Mixed       0.86      0.37      0.52        65\n",
            "        Name       0.85      0.75      0.80       133\n",
            "       Other       0.61      0.61      0.61        59\n",
            "        Tulu       0.84      0.94      0.89      1330\n",
            "         sym       1.00      1.00      1.00       455\n",
            "\n",
            "    accuracy                           0.88      3283\n",
            "   macro avg       0.86      0.74      0.78      3283\n",
            "weighted avg       0.88      0.88      0.88      3283\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9A8gPulnikQTXEGJ1CK//",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}